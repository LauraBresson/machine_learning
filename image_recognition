{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone project by Laura Bresson\n",
    "## Image recognition of fashion product images using MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from Kaggle, available [here](https://www.kaggle.com/paramaggarwal/fashion-product-images-small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Understanding the dataset](#understanding)\n",
    "2. [Extracting what's relevant](#extraction)\n",
    "3. [Building MobileNet](#building)\n",
    "4. Compiling and running MobileNet\n",
    "    \n",
    "    4.1 [locally](#local)\n",
    "    \n",
    "    4.2 [with AWS](#aws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding the dataset\n",
    "<a id=\"understanding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once unzipped, the folder containing the dataset from Kaggle contained another folder with 44,424 small (only a few kbs each) images of fashion products and a .csv file called \"styles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44424, 10)\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6044: expected 10 fields, saw 11\\nSkipping line 6569: expected 10 fields, saw 11\\nSkipping line 7399: expected 10 fields, saw 11\\nSkipping line 7939: expected 10 fields, saw 11\\nSkipping line 9026: expected 10 fields, saw 11\\nSkipping line 10264: expected 10 fields, saw 11\\nSkipping line 10427: expected 10 fields, saw 11\\nSkipping line 10905: expected 10 fields, saw 11\\nSkipping line 11373: expected 10 fields, saw 11\\nSkipping line 11945: expected 10 fields, saw 11\\nSkipping line 14112: expected 10 fields, saw 11\\nSkipping line 14532: expected 10 fields, saw 11\\nSkipping line 15076: expected 10 fields, saw 12\\nSkipping line 29906: expected 10 fields, saw 11\\nSkipping line 31625: expected 10 fields, saw 11\\nSkipping line 33020: expected 10 fields, saw 11\\nSkipping line 35748: expected 10 fields, saw 11\\nSkipping line 35962: expected 10 fields, saw 11\\nSkipping line 37770: expected 10 fields, saw 11\\nSkipping line 38105: expected 10 fields, saw 11\\nSkipping line 38275: expected 10 fields, saw 11\\nSkipping line 38404: expected 10 fields, saw 12\\n'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>masterCategory</th>\n",
       "      <th>subCategory</th>\n",
       "      <th>articleType</th>\n",
       "      <th>baseColour</th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>usage</th>\n",
       "      <th>productDisplayName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15970</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Turtle Check Men Navy Blue Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39386</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Peter England Men Party Blue Jeans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59263</td>\n",
       "      <td>Women</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Titan Women Silver Watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21379</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Track Pants</td>\n",
       "      <td>Black</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Manchester United Men Solid Black Track Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53759</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Grey T-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1855</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Tshirts</td>\n",
       "      <td>Grey</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Inkfruit Mens Chain Reaction T-shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>30805</td>\n",
       "      <td>Men</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Green</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Ethnic</td>\n",
       "      <td>Fabindia Men Striped Green Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26960</td>\n",
       "      <td>Women</td>\n",
       "      <td>Apparel</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Shirts</td>\n",
       "      <td>Purple</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Jealous 21 Women Purple Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29114</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Puma Men Pack of 3 Socks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>30039</td>\n",
       "      <td>Men</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Black</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Casual</td>\n",
       "      <td>Skagen Men Black Watch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id gender masterCategory subCategory  articleType baseColour  season  \\\n",
       "0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n",
       "1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n",
       "2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n",
       "3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n",
       "4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "5   1855    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n",
       "6  30805    Men        Apparel     Topwear       Shirts      Green  Summer   \n",
       "7  26960  Women        Apparel     Topwear       Shirts     Purple  Summer   \n",
       "8  29114    Men    Accessories       Socks        Socks  Navy Blue  Summer   \n",
       "9  30039    Men    Accessories     Watches      Watches      Black  Winter   \n",
       "\n",
       "     year   usage                             productDisplayName  \n",
       "0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  \n",
       "1  2012.0  Casual             Peter England Men Party Blue Jeans  \n",
       "2  2016.0  Casual                       Titan Women Silver Watch  \n",
       "3  2011.0  Casual  Manchester United Men Solid Black Track Pants  \n",
       "4  2012.0  Casual                          Puma Men Grey T-shirt  \n",
       "5  2011.0  Casual           Inkfruit Mens Chain Reaction T-shirt  \n",
       "6  2012.0  Ethnic               Fabindia Men Striped Green Shirt  \n",
       "7  2012.0  Casual                  Jealous 21 Women Purple Shirt  \n",
       "8  2012.0  Casual                       Puma Men Pack of 3 Socks  \n",
       "9  2016.0  Casual                         Skagen Men Black Watch  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# reading the file with the labels of our data into a pandas dataframe\n",
    "styles = pd.read_csv(filepath_or_buffer= \"styles_capstone.csv\", \\\n",
    "                     error_bad_lines= False) # the file couldn't be imported otherwise\n",
    "\n",
    "# displaying the first 10 rows of the data\n",
    "print(styles.shape)\n",
    "print(\"---\")\n",
    "styles.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this styles file indeed contains descriptive information for every image (identified with an ID). But I'd like to have a more informative look at it so creating a summary of the data would be beneficial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Color</th>\n",
       "      <th>Season</th>\n",
       "      <th>Year</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Men</td>\n",
       "      <td>Topwear</td>\n",
       "      <td>Navy Blue</td>\n",
       "      <td>Fall</td>\n",
       "      <td>2011</td>\n",
       "      <td>Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Women</td>\n",
       "      <td>Bottomwear</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2012</td>\n",
       "      <td>Ethnic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boys</td>\n",
       "      <td>Watches</td>\n",
       "      <td>Silver</td>\n",
       "      <td>Winter</td>\n",
       "      <td>2016</td>\n",
       "      <td>Formal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Girls</td>\n",
       "      <td>Socks</td>\n",
       "      <td>Black</td>\n",
       "      <td>Spring</td>\n",
       "      <td>2017</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unisex</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Grey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>None</td>\n",
       "      <td>Belts</td>\n",
       "      <td>Green</td>\n",
       "      <td>None</td>\n",
       "      <td>2014</td>\n",
       "      <td>Smart Casual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>None</td>\n",
       "      <td>Flip Flops</td>\n",
       "      <td>Purple</td>\n",
       "      <td>None</td>\n",
       "      <td>2010</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>None</td>\n",
       "      <td>Bags</td>\n",
       "      <td>White</td>\n",
       "      <td>None</td>\n",
       "      <td>2013</td>\n",
       "      <td>Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>None</td>\n",
       "      <td>Innerwear</td>\n",
       "      <td>Beige</td>\n",
       "      <td>None</td>\n",
       "      <td>2018</td>\n",
       "      <td>Home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>None</td>\n",
       "      <td>Sandal</td>\n",
       "      <td>Brown</td>\n",
       "      <td>None</td>\n",
       "      <td>2019</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>None</td>\n",
       "      <td>Shoe Accessories</td>\n",
       "      <td>Bronze</td>\n",
       "      <td>None</td>\n",
       "      <td>2007</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>None</td>\n",
       "      <td>Fragrance</td>\n",
       "      <td>Teal</td>\n",
       "      <td>None</td>\n",
       "      <td>2009</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>None</td>\n",
       "      <td>Jewellery</td>\n",
       "      <td>Copper</td>\n",
       "      <td>None</td>\n",
       "      <td>2008</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>None</td>\n",
       "      <td>Lips</td>\n",
       "      <td>Pink</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>None</td>\n",
       "      <td>Saree</td>\n",
       "      <td>Off White</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>None</td>\n",
       "      <td>Eyewear</td>\n",
       "      <td>Maroon</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>None</td>\n",
       "      <td>Nails</td>\n",
       "      <td>Red</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>None</td>\n",
       "      <td>Scarves</td>\n",
       "      <td>Khaki</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>None</td>\n",
       "      <td>Dress</td>\n",
       "      <td>Orange</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>None</td>\n",
       "      <td>Loungewear and Nightwear</td>\n",
       "      <td>Coffee Brown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>None</td>\n",
       "      <td>Wallets</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>None</td>\n",
       "      <td>Apparel Set</td>\n",
       "      <td>Charcoal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>None</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Gold</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>None</td>\n",
       "      <td>Mufflers</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>None</td>\n",
       "      <td>Skin Care</td>\n",
       "      <td>Tan</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>None</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>Multi</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>None</td>\n",
       "      <td>Free Gifts</td>\n",
       "      <td>Magenta</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>None</td>\n",
       "      <td>Ties</td>\n",
       "      <td>Lavender</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>None</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Sea Green</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>None</td>\n",
       "      <td>Skin</td>\n",
       "      <td>Cream</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>None</td>\n",
       "      <td>Beauty Accessories</td>\n",
       "      <td>Peach</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>None</td>\n",
       "      <td>Water Bottle</td>\n",
       "      <td>Olive</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>None</td>\n",
       "      <td>Eyes</td>\n",
       "      <td>Skin</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>None</td>\n",
       "      <td>Bath and Body</td>\n",
       "      <td>Burgundy</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>None</td>\n",
       "      <td>Gloves</td>\n",
       "      <td>Grey Melange</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>None</td>\n",
       "      <td>Sports Accessories</td>\n",
       "      <td>Rust</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>None</td>\n",
       "      <td>Cufflinks</td>\n",
       "      <td>Rose</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>None</td>\n",
       "      <td>Sports Equipment</td>\n",
       "      <td>Lime Green</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>None</td>\n",
       "      <td>Stoles</td>\n",
       "      <td>Mauve</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>None</td>\n",
       "      <td>Hair</td>\n",
       "      <td>Turquoise Blue</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>None</td>\n",
       "      <td>Perfumes</td>\n",
       "      <td>Metallic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>None</td>\n",
       "      <td>Home Furnishing</td>\n",
       "      <td>Mustard</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>None</td>\n",
       "      <td>Umbrellas</td>\n",
       "      <td>Taupe</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>None</td>\n",
       "      <td>Wristbands</td>\n",
       "      <td>Nude</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>None</td>\n",
       "      <td>Vouchers</td>\n",
       "      <td>Mushroom Brown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Fluorescent Green</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Gender               Subcategory              Color  Season  Year  \\\n",
       "0      Men                   Topwear          Navy Blue    Fall  2011   \n",
       "1    Women                Bottomwear               Blue  Summer  2012   \n",
       "2     Boys                   Watches             Silver  Winter  2016   \n",
       "3    Girls                     Socks              Black  Spring  2017   \n",
       "4   Unisex                     Shoes               Grey     NaN  2015   \n",
       "5     None                     Belts              Green    None  2014   \n",
       "6     None                Flip Flops             Purple    None  2010   \n",
       "7     None                      Bags              White    None  2013   \n",
       "8     None                 Innerwear              Beige    None  2018   \n",
       "9     None                    Sandal              Brown    None  2019   \n",
       "10    None          Shoe Accessories             Bronze    None  2007   \n",
       "11    None                 Fragrance               Teal    None  2009   \n",
       "12    None                 Jewellery             Copper    None  2008   \n",
       "13    None                      Lips               Pink    None   NaN   \n",
       "14    None                     Saree          Off White    None  None   \n",
       "15    None                   Eyewear             Maroon    None  None   \n",
       "16    None                     Nails                Red    None  None   \n",
       "17    None                   Scarves              Khaki    None  None   \n",
       "18    None                     Dress             Orange    None  None   \n",
       "19    None  Loungewear and Nightwear       Coffee Brown    None  None   \n",
       "20    None                   Wallets             Yellow    None  None   \n",
       "21    None               Apparel Set           Charcoal    None  None   \n",
       "22    None                  Headwear               Gold    None  None   \n",
       "23    None                  Mufflers              Steel    None  None   \n",
       "24    None                 Skin Care                Tan    None  None   \n",
       "25    None                    Makeup              Multi    None  None   \n",
       "26    None                Free Gifts            Magenta    None  None   \n",
       "27    None                      Ties           Lavender    None  None   \n",
       "28    None               Accessories          Sea Green    None  None   \n",
       "29    None                      Skin              Cream    None  None   \n",
       "30    None        Beauty Accessories              Peach    None  None   \n",
       "31    None              Water Bottle              Olive    None  None   \n",
       "32    None                      Eyes               Skin    None  None   \n",
       "33    None             Bath and Body           Burgundy    None  None   \n",
       "34    None                    Gloves       Grey Melange    None  None   \n",
       "35    None        Sports Accessories               Rust    None  None   \n",
       "36    None                 Cufflinks               Rose    None  None   \n",
       "37    None          Sports Equipment         Lime Green    None  None   \n",
       "38    None                    Stoles              Mauve    None  None   \n",
       "39    None                      Hair     Turquoise Blue    None  None   \n",
       "40    None                  Perfumes           Metallic    None  None   \n",
       "41    None           Home Furnishing            Mustard    None  None   \n",
       "42    None                 Umbrellas              Taupe    None  None   \n",
       "43    None                Wristbands               Nude    None  None   \n",
       "44    None                  Vouchers     Mushroom Brown    None  None   \n",
       "45     NaN                       NaN                NaN     NaN   NaN   \n",
       "46    None                      None  Fluorescent Green    None  None   \n",
       "\n",
       "           Usage  \n",
       "0         Casual  \n",
       "1         Ethnic  \n",
       "2         Formal  \n",
       "3         Sports  \n",
       "4            NaN  \n",
       "5   Smart Casual  \n",
       "6         Travel  \n",
       "7          Party  \n",
       "8           Home  \n",
       "9           None  \n",
       "10          None  \n",
       "11          None  \n",
       "12          None  \n",
       "13          None  \n",
       "14          None  \n",
       "15          None  \n",
       "16          None  \n",
       "17          None  \n",
       "18          None  \n",
       "19          None  \n",
       "20          None  \n",
       "21          None  \n",
       "22          None  \n",
       "23          None  \n",
       "24          None  \n",
       "25          None  \n",
       "26          None  \n",
       "27          None  \n",
       "28          None  \n",
       "29          None  \n",
       "30          None  \n",
       "31          None  \n",
       "32          None  \n",
       "33          None  \n",
       "34          None  \n",
       "35          None  \n",
       "36          None  \n",
       "37          None  \n",
       "38          None  \n",
       "39          None  \n",
       "40          None  \n",
       "41          None  \n",
       "42          None  \n",
       "43          None  \n",
       "44          None  \n",
       "45           NaN  \n",
       "46          None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a summary of our dataset to better understand its features\n",
    "# the most relevant ones are : gender, subCategory, colour, season, year and usage\n",
    "summary = pd.DataFrame(data= [styles.gender.unique(), \\\n",
    "                               styles.subCategory.unique(), \\\n",
    "                               styles.baseColour.unique(), \\\n",
    "                               styles.season.unique(), \\\n",
    "                               styles.year.unique(), \\\n",
    "                               styles.usage.unique()]).transpose()\n",
    "summary.columns = [\"Gender\", \"Subcategory\", \"Color\", \"Season\", \"Year\", \"Usage\"]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gender          6\n",
       "Subcategory    46\n",
       "Color          46\n",
       "Season          5\n",
       "Year           15\n",
       "Usage           9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glossary.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that our dataset comprises a number of different features : \n",
    "- different genders : man, woman, boy, girl, unisex\n",
    "- 46 different subcategories\n",
    "- 46 different colors\n",
    "- all four seasons\n",
    "- spanning from 2007 to 2019\n",
    "- 9 different usages from casual to travel\n",
    "\n",
    "Being interested in building a neural network to recognize fashion product images, I'm only interested in the subCategory column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Topwear                     15402\n",
       "Shoes                        7343\n",
       "Bags                         3055\n",
       "Bottomwear                   2694\n",
       "Watches                      2542\n",
       "Innerwear                    1808\n",
       "Jewellery                    1079\n",
       "Eyewear                      1073\n",
       "Fragrance                    1011\n",
       "Sandal                        963\n",
       "Wallets                       933\n",
       "Flip Flops                    913\n",
       "Belts                         811\n",
       "Socks                         698\n",
       "Lips                          527\n",
       "Dress                         478\n",
       "Loungewear and Nightwear      470\n",
       "Saree                         427\n",
       "Nails                         329\n",
       "Makeup                        307\n",
       "Headwear                      293\n",
       "Ties                          258\n",
       "Accessories                   129\n",
       "Scarves                       118\n",
       "Cufflinks                     108\n",
       "Apparel Set                   106\n",
       "Free Gifts                    104\n",
       "Stoles                         90\n",
       "Skin Care                      77\n",
       "Skin                           69\n",
       "Eyes                           43\n",
       "Mufflers                       38\n",
       "Shoe Accessories               24\n",
       "Sports Equipment               21\n",
       "Gloves                         20\n",
       "Hair                           19\n",
       "Bath and Body                  12\n",
       "Water Bottle                    7\n",
       "Perfumes                        6\n",
       "Umbrellas                       6\n",
       "Beauty Accessories              4\n",
       "Wristbands                      4\n",
       "Sports Accessories              3\n",
       "Vouchers                        1\n",
       "Home Furnishing                 1\n",
       "Name: subCategory, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting the number of the different values in the subCategory column\n",
    "styles['subCategory'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can see that the most prominent articles in the subCategory column are \n",
    "- topwear with 15k+ images\n",
    "- shoes with 7.3k+ images\n",
    "- bags with 3k+ images\n",
    "- bottomwear with 2.6k+ images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extracting what is relevant from the dataset\n",
    "<a id=\"extraction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize the number of images my neural network can be trained on, I'll use the four most important categories of images discovered above. \n",
    "\n",
    "I need to move these specific images from their original folder to a training data folder I'll be able to use for my neural network. \n",
    "\n",
    "For that, I'll be using a bash script to have the Terminal move these files for me (18k+ images forbids a manual operation). Each image is associated with an ID in the styles dataframe, which ID corresponding to the filename of each image. So by extracting from the dataframe the ID of the images I want I'll be able to have the filenames of the images to move. \n",
    "\n",
    "In order to have these lists of filnames, I'll need :\n",
    "1. isolate the rows with \"bottomwear, topwear, shoes or bags\" as the subCategory\n",
    "2. isolate even further to just get the IDs\n",
    "3. transform these lists into files themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolating the rows with Bottomwear in the column subCategory\n",
    "bottomwear = styles[styles['subCategory'].isin([\"Bottomwear\"])]\n",
    "\n",
    "# isolating the rows with Topwear in the column subCategory\n",
    "topwear = styles[styles['subCategory'].isin([\"Topwear\"])]\n",
    "\n",
    "# isolating the rows with Shoes in the column subCategory\n",
    "shoes = styles[styles['subCategory'].isin([\"Shoes\"])]\n",
    "\n",
    "# isolating the rows with Bags in the column subCategory\n",
    "bags = styles[styles['subCategory'].isin([\"Bags\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating my lists of image IDs for my images \n",
    "bottomwear_list = bottomwear['id']\n",
    "topwear_list = topwear['id']\n",
    "shoes_list = shoes['id']\n",
    "bags_list = bags['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating files with the IDs\n",
    "bottomwear_list.to_csv(r'/Users/laurabresson/Jupyter_notebooks/Personal_projects/BrainStation_capstone/bottomwear_list.csv', \\\n",
    "                       index= False)\n",
    "topwear_list.to_csv(r'/Users/laurabresson/Jupyter_notebooks/Personal_projects/BrainStation_capstone/topwear_list.csv',\\\n",
    "                   index= False)\n",
    "shoes_list.to_csv(r'/Users/laurabresson/Jupyter_notebooks/Personal_projects/BrainStation_capstone/shoes_list.csv',\\\n",
    "                 index= False)\n",
    "bags_list.to_csv(r'/Users/laurabresson/Jupyter_notebooks/Personal_projects/BrainStation_capstone/bags_list.csv',\\ \n",
    "                 index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the bash script, I was able to move my images of interest to a folder of their own to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Building a Convolutional Neural Network (CNN) with Keras\n",
    "<a id=\"building\"></a>\n",
    "\n",
    "Convolutional Neural Networks (CNN) are the most appropriate neural networks to use on images and have for example been used in the software of self-driving cars in the process of segmentation as shown below. \n",
    "\n",
    "<img src=\"self_driving.png\" />\n",
    "\n",
    "The reason behind the wide use of CNN for the treatment of images lies in their ability to \"summarize\" an image by using a window, or \"kernel\", and extract the features from the image one step at a time, creating a \"convolved feature\". \n",
    "\n",
    "<img src=\"convolution_schematic.gif\" />\n",
    "\n",
    "One of these CNN, which we studied, is called MobileNet and it is the one I'll be building and working with for this capstone project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# being able to process image data\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# using the pre trained model MobileNet\n",
    "from keras.applications import MobileNet\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "# tuning the model to our needs\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0830 13:24:51.274242 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0830 13:24:51.362682 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0830 13:24:51.400220 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0830 13:24:51.448350 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0830 13:24:51.449540 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0830 13:24:51.635286 4546250176 deprecation_wrapper.py:119] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign the MobileNet blueprint to a variable\n",
    "    # with the weights optimized on the imagenet dataset\n",
    "    # but without the output layer (include_top = False)\n",
    "blueprint = MobileNet(weights= \"imagenet\", include_top= False)\n",
    "\n",
    "# assign the output of this blueprint to a variable\n",
    "my_layers = blueprint.output\n",
    "\n",
    "# add a pooling layer at the end of the blueprint \n",
    "    # the GlobalAveragePooling2D is recommended for image data\n",
    "my_layers = GlobalAveragePooling2D()(my_layers)\n",
    "\n",
    "# add three dense layers (fully connected layers) \n",
    "    # so that the model can learn about our dataset\n",
    "    # units = nodes \n",
    "    # activation is by default none \n",
    "        # relu is the best activation out there currently\n",
    "my_layers = Dense(units= 243, activation= \"relu\")(my_layers)\n",
    "my_layers = Dense(units= 243, activation= \"relu\")(my_layers)\n",
    "my_layers = Dense(units= 81, activation= \"relu\")(my_layers)\n",
    "\n",
    "# adding an output layer with 4 nodes (our number of classes)\n",
    "    # using a softmax function for classification\n",
    "output_layer = Dense(units= 4, activation= \"softmax\")(my_layers)\n",
    "\n",
    "# instantiate this model wth specified inputs and outputs\n",
    "model = Model(inputs= blueprint.input, \n",
    "              outputs= output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, None, None, 32)    864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, None, None, 32)    288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, None, None, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, None, None, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, None, None, 64)    2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, None, None, 64)    576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, None, None, 128)   8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, None, None, 128)   16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, None, None, 128)   512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, None, None, 256)   32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, None, None, 256)   65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, None, None, 512)   131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, None, None, 512)   262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, None, None, 1024)  524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, None, None, 1024)  9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, None, None, 1024)  1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 243)               249075    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 243)               59292     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 81)                19764     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 328       \n",
      "=================================================================\n",
      "Total params: 3,557,323\n",
      "Trainable params: 3,535,435\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# getting the summary of this model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this module creates an image file so we can visualize our model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model= model, # passing in the name of our model \n",
    "           to_file= \"model.png\", # naming the file being created\n",
    "           show_shapes= True, # showing the shapes of our layers\n",
    "          show_layer_names= True, # as well as their names\n",
    "          rankdir= \"LR\") # whether to display the model structure\n",
    "                         # vertically (TR) or horizontally (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the layers from the blueprint to be static\n",
    "for layer in model.layers[:87]:\n",
    "    layer.trainable= False\n",
    "    \n",
    "# setting my own layers to be trainable\n",
    "for layer in model.layers[87:]:\n",
    "    layer.trainable= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19945 images belonging to 4 classes.\n",
      "Found 8544 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating a data generator we can then feed into our model\n",
    "# we need to specify the preprocessing function \n",
    "# so that our data is compatible with our model requirements\n",
    "data_feed = ImageDataGenerator(preprocessing_function= preprocess_input, validation_split= 0.3)\n",
    "\n",
    "# creating our training data flow from our file system\n",
    "train_data = data_feed.flow_from_directory(directory= \"train/\", \n",
    "                                    target_size= (224, 224),\n",
    "                                    color_mode= \"rgb\",\n",
    "                                    batch_size= 256,\n",
    "                                    class_mode= \"categorical\",\n",
    "                                    shuffle= True,\n",
    "                                    subset= \"training\")\n",
    "\n",
    "val_data = data_feed.flow_from_directory(directory= \"train/\",\n",
    "                                        target_size= (224,224),\n",
    "                                        color_mode= \"rgb\",\n",
    "                                        batch_size= 256,\n",
    "                                        class_mode= \"categorical\",\n",
    "                                        shuffle= True,\n",
    "                                        subset= \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving the names of the classes\n",
      "{'bags': 0, 'bottomwear': 1, 'shoes': 2, 'topwear': 3}\n",
      "---\n",
      "Using a LabelEncoder\n",
      "['bags' 'bottomwear' 'shoes' 'topwear']\n",
      "---\n",
      "Verifying the encoding\n",
      "0: bags\n",
      "1: bottomwear\n",
      "2: shoes\n",
      "3: topwear\n"
     ]
    }
   ],
   "source": [
    "# retrieving the names of the classes from our \"engine\" (=data)\n",
    "print(\"Retrieving the names of the classes\")\n",
    "label_map = train_data.class_indices\n",
    "print(label_map)\n",
    "print(\"---\")\n",
    "\n",
    "# however the best way to have our labels is to use an encoder\n",
    "    # this will be useful later when getting the predictions out\n",
    "    # and testing our model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_code = LabelEncoder()\n",
    "label_code.fit(list(label_map.keys()))\n",
    "print(\"Using a LabelEncoder\")\n",
    "print(label_code.classes_)\n",
    "print(\"---\")\n",
    "\n",
    "# verifying the encoding\n",
    "print(\"Verifying the encoding\")\n",
    "for i, j in enumerate(label_code.classes_):\n",
    "    print(f\"{i}: {j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last few cells, we've : \n",
    "- instantiated the MobileNet model without its final layers so that we could recreate those final layers to suit our own needs\n",
    "- merged together this bare model with newly created layers\n",
    "- visualized both here and in a .png file the structure of our model\n",
    "- only allowed our layers to be trainable on our dataset\n",
    "- used an ImageDataGenerator to extract our image files into training and validation data with a 70/30% split\n",
    "- encoded our labels into a labelencoder for further use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### 4. Running the model\n",
    "### 4.1 locally\n",
    "<a id=\"local\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0826 18:42:28.244050 4553283008 deprecation.py:323] From /Users/laurabresson/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17/77 [=====>........................] - ETA: 25:58 - loss: 0.4260 - acc: 0.8412"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9e26c0e3d788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# passing our validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# what is the step size for our validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                             callbacks= [checkpoints, early_stop]) # a list of our previously defined callbacks\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# compiling our model : optimizer, loss, metric\n",
    "model.compile(optimizer= \"Adam\", # Adam is the best for large data\n",
    "              loss= \"categorical_crossentropy\", # classes are categorical\n",
    "              metrics= [\"accuracy\"]) # loss could be used as a metric as well\n",
    "              \n",
    "# when flow_from_directory is used, there is need for a step_size\n",
    "    # step_size is calculated with n° data points divided by batch_size\n",
    "    # both were specified when data was loaded \n",
    "step_size = train_data.n//train_data.batch_size\n",
    "\n",
    "# defining checkpoints (to save our progress in the training) \n",
    "# and an early stop (no need to keep running if there's no improvement\n",
    "# making sure that the launch is efficient\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(filepath= \"/Users/laurabresson/Jupyter_notebooks/Personal_projects/BrainStation_capstone/\", \n",
    "                                            monitor= \"acc\", # the value to monitor is the loss\n",
    "                                            verbose= 1, # set to be verbose about results\n",
    "                                            save_best_only= True, # saves only the best model on file\n",
    "                                            mode= \"max\") # the goal is to minimize the loss\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor= \"acc\", # the early stopping monitors the accuracy \n",
    "                                          min_delta= 0.02, # the accuracy must be improve by at least 0.02 (2%) each epoch\n",
    "                                          patience= 1, # the callback will wait for a full epoch before stopping\n",
    "                                          verbose= 1, # set to be verbose\n",
    "                                          mode= \"max\") # the goal is to maximize the accuracy\n",
    "\n",
    "# now lauching this baby \n",
    "launch = model.fit_generator(generator= train_data, # the generator is the training phase so it needs our training data\n",
    "                            steps_per_epoch= step_size, # what is the step size in our training\n",
    "                            epochs= 10, # number of epochs to train our model\n",
    "                            validation_data= val_data, # passing our validation data\n",
    "                            validation_steps= step_size, # what is the step size for our validation data\n",
    "                            callbacks= [checkpoints, early_stop]) # a list of our previously defined callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a dataset of size 28k+, I need to have more computing power as potentially 10+ hours (estimated one hour per epoch) to train one model is something I do not want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 With AWS\n",
    "<a id=\"aws\"></a>\n",
    "\n",
    "Using a personal AWS account equipped with an increased access to powerful instances, I could use a Graphic Processing Unit (GPU p2.xlarge to be exact) to run my model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "77/77 [==============================] - 128s 2s/step - loss: 0.0741 - acc: 0.9812 - val_loss: 0.0239 - val_acc: 0.9925\n",
      "\n",
      "Epoch 00001: acc improved from -inf to 0.98118, saving model to checkpoints.hdf5\n",
      "Epoch 2/10\n",
      "77/77 [==============================] - 122s 2s/step - loss: 0.0216 - acc: 0.9943 - val_loss: 0.0280 - val_acc: 0.9907\n",
      "\n",
      "Epoch 00002: acc improved from 0.98118 to 0.99430, saving model to checkpoints.hdf5\n",
      "Epoch 00002: early stopping\n"
     ]
    }
   ],
   "source": [
    "# compiling our model : optimizer, loss, metric\n",
    "model.compile(optimizer= \"Adam\", # Adam is the best for large data\n",
    "              loss= \"categorical_crossentropy\", # classes are categorical\n",
    "              metrics= [\"accuracy\"]) # loss could be used as a metric as well\n",
    "              \n",
    "# when flow_from_directory is used, there is need for a step_size\n",
    "    # step_size is calculated with n° data points divided by batch_size\n",
    "    # both were specified when data was loaded \n",
    "step_size = train_data.n//train_data.batch_size\n",
    "\n",
    "# defining checkpoints (to save our progress in the training) \n",
    "# and an early stop (no need to keep running if there's no improvement\n",
    "# making sure that the launch is efficient\n",
    "checkpoints = keras.callbacks.ModelCheckpoint(filepath= \"checkpoints.hdf5\", \n",
    "                                            monitor= \"acc\", # the value to monitor is the loss\n",
    "                                            verbose= 1, # set to be verbose about results\n",
    "                                            save_best_only= True, # saves only the best model on file\n",
    "                                            mode= \"max\") # the goal is to minimize the loss\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor= \"acc\", # the early stopping monitors the accuracy \n",
    "                                          min_delta= 0.02, # the accuracy must be improve by at least 0.02 (2%) each epoch\n",
    "                                          patience= 1, # the callback will wait for a full epoch before stopping\n",
    "                                          verbose= 1, # set to be verbose\n",
    "                                          mode= \"max\") # the goal is to maximize the accuracy\n",
    "\n",
    "# now lauching this baby \n",
    "launch = model.fit_generator(generator= train_data, # the generator is the training phase so it needs our training data\n",
    "                            steps_per_epoch= step_size, # what is the step size in our training\n",
    "                            epochs= 10, # number of epochs to train our model\n",
    "                            validation_data= val_data, # passing our validation data\n",
    "                            validation_steps= step_size, # what is the step size for our validation data\n",
    "                            callbacks= [checkpoints, early_stop]) # a list of our previously defined callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the early stopping function available in Keras, the model didn't run for more than two epochs, hence saving valuable time and resources. \n",
    "\n",
    "Thanks to the GPU, what would have taken my computer 2+ hours to locally run only took 4.16 minutes to run this time. \n",
    "\n",
    "The final val_acc (the validation accuracy) is of .9907 which means that the model had a validation accuracy of 99.07%, which is good news. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
